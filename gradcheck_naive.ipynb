{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gradcheck_naive.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidkorea/google_colaboratory/blob/master/gradcheck_naive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sniMjnPG0v48",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PUeFz_jX1JGQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "  对一个函数f求梯度的梯度检验 \n",
        "  - f 输入x，然后输出loss和梯度的函数,  // 输出(函数值，导数)\n",
        "  - x 就是输入咯"
      ]
    },
    {
      "metadata": {
        "id": "Boy6VG8g1BZz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gradcheck_naive(f, x):\n",
        "    \"\"\" Gradient check for a function f.\n",
        "\n",
        "    Arguments:\n",
        "    f -- a function that takes a single argument and outputs the\n",
        "         cost and its gradients\n",
        "    x -- the point (numpy array) to check the gradient at\n",
        "    \"\"\"\n",
        "\n",
        "    rndstate = random.getstate()\n",
        "    random.setstate(rndstate)\n",
        "    fx, grad = f(x) # Evaluate function value at original point\n",
        "    h = 1e-4        # Do not change this!\n",
        "\n",
        "    # Iterate over all indexes ix in x to check the gradient.\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        ix = it.multi_index\n",
        "\n",
        "        # Try modifying x[ix] with h defined above to compute numerical\n",
        "        # gradients (numgrad).\n",
        "\n",
        "        # Use the centered difference of the gradient.\n",
        "        # It has smaller asymptotic error than forward / backward difference\n",
        "        # methods. If you are curious, check out here:\n",
        "        # https://math.stackexchange.com/questions/2326181/when-to-use-forward-or-central-difference-approximations\n",
        "\n",
        "        # Make sure you call random.setstate(rndstate)\n",
        "        # before calling f(x) each time. This will make it possible\n",
        "        # to test cost functions with built in randomness later.\n",
        "\n",
        "        ### YOUR CODE HERE:\n",
        "        # raise NotImplementedError\n",
        "        # 对矩阵中的每一个元素，+一个很小的数，在-一个很小的数，除以2倍这个很小的数。\n",
        "        # 再还原该元素在矩阵中的原始值\n",
        "        old_val = x[ix]\n",
        "        x[ix] = old_val - h\n",
        "        random.setstate(rndstate)\n",
        "        ( fxh1, _ ) = f(x)\n",
        "        \n",
        "        x[ix] = old_val + h\n",
        "        random.setstate(rndstate)\n",
        "        ( fxh2, _ ) = f(x)\n",
        "\n",
        "        numgrad = (fxh2 - fxh1)/(2*h)\n",
        "        x[ix] = old_val\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "        # Compare gradients\n",
        "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
        "        if reldiff > 1e-5:\n",
        "            print (\"Gradient check failed.\")\n",
        "            print (\"First gradient error found at index %s\" % str(ix))\n",
        "            print (\"Your gradient: %f \\t Numerical gradient: %f\" % (\n",
        "                grad[ix], numgrad))\n",
        "            return\n",
        "\n",
        "        it.iternext() # Step to next dimension\n",
        "\n",
        "    print (\"Gradient check passed!\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0fEZj-f63TxE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sanity_check():\n",
        "    \"\"\"\n",
        "    Some basic sanity checks.\n",
        "    \"\"\"\n",
        "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
        "\n",
        "    print (\"Running sanity checks...\")\n",
        "    gradcheck_naive(quad, np.array(123.456))      # scalar test\n",
        "    gradcheck_naive(quad, np.random.randn(3,))    # 1-D test\n",
        "    gradcheck_naive(quad, np.random.randn(4,5))   # 2-D test\n",
        "    print (\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fNJraTZl3rMV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "bd0bee62-aa5f-4539-d4db-272392f37043"
      },
      "cell_type": "code",
      "source": [
        "sanity_check()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running sanity checks...\n",
            "Gradient check passed!\n",
            "Gradient check passed!\n",
            "Gradient check passed!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "233JOi6w3whb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}