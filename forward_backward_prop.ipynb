{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "forward_backward_prop.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidkorea/google_colaboratory/blob/master/forward_backward_prop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uXUH1i1W-Lku",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "12c1ENY--WPD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N = 20\n",
        "dimensions = [10, 5, 10]\n",
        "data = np.random.randn(N, dimensions[0])   # each row will be a datum\n",
        "labels = np.zeros((N, dimensions[2]))\n",
        "for i in range(N):\n",
        "    labels[i, random.randint(0,dimensions[2]-1)] = 1\n",
        "\n",
        "params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (\n",
        "    dimensions[1] + 1) * dimensions[2], )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hzEecWMF_TKV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1e22abf2-4bd6-4422-cd7c-7f01e21c1aaa"
      },
      "cell_type": "code",
      "source": [
        "(dimensions[0] + 1) * dimensions[1] + (\n",
        "    dimensions[1] + 1) * dimensions[2]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "115"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "GJ7ddyIv-aBQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "555c984e-90ef-4292-be9a-bfcf3a2d7430"
      },
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "EpfsxOZo-oe9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "f74310ae-8b5a-4513-b874-f78932b97104"
      },
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "Gd58TTOw-yaV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7a62a79b-b33b-42cf-f5e3-1e6ae8175c40"
      },
      "cell_type": "code",
      "source": [
        "params.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(115,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "wCbIHG4z_ISs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "80b85b1e-b44c-4173-e38f-0ead080d62f0"
      },
      "cell_type": "code",
      "source": [
        "params"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3.50149115e-01,  1.10782969e+00,  8.46282871e-01, -2.12331050e-01,\n",
              "       -7.18801999e-01,  2.41575648e+00, -4.25350598e-01, -6.34230923e-01,\n",
              "        5.67461363e-02, -4.21419311e-01,  3.14121654e-01, -1.56601316e+00,\n",
              "        6.70359676e-01, -8.98870229e-01,  1.67312956e+00, -1.72370731e+00,\n",
              "       -1.13225767e+00,  5.96975320e-01, -2.54969604e-01,  9.34719826e-01,\n",
              "        1.26775007e-01,  2.99160768e-01, -1.14151698e-01,  6.15052814e-01,\n",
              "        2.30617104e+00, -5.89336736e-01, -6.62581000e-02, -5.93743464e-01,\n",
              "       -1.26097556e+00,  4.43998139e-01, -7.69904124e-01,  6.86751335e-01,\n",
              "        4.88354331e-02,  1.28084297e+00, -1.42302147e+00, -5.53330278e-01,\n",
              "       -4.31515353e-01,  9.95691809e-01,  5.78719971e-01,  1.04663424e+00,\n",
              "        7.36117949e-01, -7.63258229e-01, -1.72386277e+00,  1.99223582e-01,\n",
              "        2.03256132e+00, -1.75120825e+00,  3.83870222e-01, -4.63779262e-01,\n",
              "        2.12580257e-01, -2.10892375e-01,  8.26726352e-01, -2.87825635e-01,\n",
              "        1.15888911e+00, -4.47091741e-01, -7.25528746e-01,  1.45746822e+00,\n",
              "        1.95225330e-03, -4.72518918e-01, -4.14215504e-01,  4.09243656e-02,\n",
              "       -1.82066779e+00,  2.21266964e-01, -8.79642388e-01,  1.00762291e+00,\n",
              "       -4.74108273e-01,  2.83524924e+00,  2.84488691e-02, -2.55864561e-03,\n",
              "        6.80687949e-01, -5.75110441e-01,  1.16570085e+00,  2.62555650e-02,\n",
              "       -3.94958502e-01, -8.96355330e-01, -3.01489939e-01, -1.36219746e+00,\n",
              "       -5.71386792e-01,  1.45899898e+00,  6.11743552e-01, -1.21642018e+00,\n",
              "        2.20580767e-01, -1.65938365e+00,  5.57834097e-01,  5.12661326e-01,\n",
              "        1.03020984e+00,  1.26810276e+00,  9.75656285e-01, -1.07993080e-01,\n",
              "       -6.71799184e-02, -1.70912989e+00,  1.66438668e+00, -7.23010973e-01,\n",
              "       -1.37457091e-01, -9.21219006e-01,  1.54495263e+00,  6.41074219e-01,\n",
              "       -2.33897637e-01, -9.08853682e-01,  4.08728417e-02, -2.51989610e-01,\n",
              "       -5.73700400e-01,  1.28388598e+00,  3.42839263e-01, -2.96472675e-02,\n",
              "       -8.55659201e-01, -1.79798030e+00, -1.95130888e+00, -4.91705583e-01,\n",
              "       -7.97573708e-01, -9.19353104e-01,  1.17370981e+00,  5.88704199e-01,\n",
              "        2.75544418e-01, -1.00731303e+00, -1.44900360e-01])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "6U76tI-E_WzU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9ccd359d-53c2-4fec-bc5d-5bed661ada55"
      },
      "cell_type": "code",
      "source": [
        "np.reshape(params[0:6],(2,3))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.35014911,  1.10782969,  0.84628287],\n",
              "       [-0.21233105, -0.718802  ,  2.41575648]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "QKxwrGfH_sPM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ActqDCVfl9jv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gradcheck_naive(f, x):\n",
        "    \"\"\" Gradient check for a function f.\n",
        "\n",
        "    Arguments:\n",
        "    f -- a function that takes a single argument and outputs the\n",
        "         cost and its gradients\n",
        "    x -- the point (numpy array) to check the gradient at\n",
        "    \"\"\"\n",
        "\n",
        "    rndstate = random.getstate()\n",
        "    random.setstate(rndstate)\n",
        "#     fx, grad = f(x) # Evaluate function value at original point\n",
        "    h = 1e-4        # Do not change this!\n",
        "\n",
        "    # Iterate over all indexes ix in x to check the gradient.\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        ix = it.multi_index\n",
        "\n",
        "        # Try modifying x[ix] with h defined above to compute numerical\n",
        "        # gradients (numgrad).\n",
        "\n",
        "        # Use the centered difference of the gradient.\n",
        "        # It has smaller asymptotic error than forward / backward difference\n",
        "        # methods. If you are curious, check out here:\n",
        "        # https://math.stackexchange.com/questions/2326181/when-to-use-forward-or-central-difference-approximations\n",
        "\n",
        "        # Make sure you call random.setstate(rndstate)\n",
        "        # before calling f(x) each time. This will make it possible\n",
        "        # to test cost functions with built in randomness later.\n",
        "\n",
        "        ### YOUR CODE HERE:\n",
        "        # raise NotImplementedError\n",
        "        # 对矩阵中的每一个元素，+一个很小的数，在-一个很小的数，除以2倍这个很小的数。\n",
        "        # 再还原该元素在矩阵中的原始值\n",
        "        old_val = x[ix]\n",
        "        x[ix] = old_val - h\n",
        "        random.setstate(rndstate)\n",
        "        fxh1 = f(x)\n",
        "        \n",
        "        x[ix] = old_val + h\n",
        "        random.setstate(rndstate)\n",
        "        fxh2 = f(x)\n",
        "\n",
        "        numgrad = (fxh2 - fxh1)/(2*h)\n",
        "        x[ix] = old_val\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "        # Compare gradients\n",
        "#         reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
        "#         if reldiff > 1e-5:\n",
        "#             print (\"Gradient check failed.\")\n",
        "#             print (\"First gradient error found at index %s\" % str(ix))\n",
        "#             print (\"Your gradient: %f \\t Numerical gradient: %f\" % (\n",
        "#                 grad[ix], numgrad))\n",
        "#             return\n",
        "\n",
        "        it.iternext() # Step to next dimension\n",
        "\n",
        "#     print (\"Gradient check passed!\")\n",
        "    return numgrad\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3N1IYAwIl9bL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid function for the input here.\n",
        "\n",
        "    Arguments:\n",
        "    x -- A scalar or numpy array.\n",
        "\n",
        "    Return:\n",
        "    s -- sigmoid(x)\n",
        "    \"\"\"\n",
        "\n",
        "    s = 1 / (1 + np.exp(-x))\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Hny32PDl9UE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Compute the softmax function for each row of the input x.\n",
        "\n",
        "    It is crucial that this function is optimized for speed because\n",
        "    it will be used frequently in later code. You might find numpy\n",
        "    functions np.exp, np.sum, np.reshape, np.max, and numpy\n",
        "    broadcasting useful for this task.\n",
        "\n",
        "    Numpy broadcasting documentation:\n",
        "    http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\n",
        "\n",
        "    You should also make sure that your code works for a single\n",
        "    D-dimensional vector (treat the vector as a single row) and\n",
        "    for N x D matrices. This may be useful for testing later. Also,\n",
        "    make sure that the dimensions of the output match the input.\n",
        "\n",
        "    You must implement the optimization in problem 1(a) of the\n",
        "    written assignment!\n",
        "\n",
        "    Arguments:\n",
        "    x -- A D dimensional vector or N x D dimensional numpy matrix.\n",
        "\n",
        "    Return:\n",
        "    x -- You are allowed to modify x in-place\n",
        "    \"\"\"\n",
        "    orig_shape = x.shape\n",
        "    soft_max_list = []\n",
        "    if len(x.shape) > 1:\n",
        "        # Matrix\n",
        "        ### YOUR CODE HERE\n",
        "        for i in range(x.shape[0]): #rows\n",
        "          soft_max_row = []\n",
        "          row = x[i]\n",
        "          argmax = np.argmax(row)\n",
        "          max_i = row[argmax]\n",
        "          exp_max_row = np.exp(row-max_i)\n",
        "          down = np.sum(exp_max_row)\n",
        "          for j in range(x.shape[1]):\n",
        "            up = exp_max_row[j]\n",
        "            soft_max_i = up / down\n",
        "            soft_max_row.append(soft_max_i)\n",
        "          soft_max_list.append(soft_max_row)\n",
        "#         raise NotImplementedError\n",
        "        ### END YOUR CODE\n",
        "    else:\n",
        "        # Vector\n",
        "        ### YOUR CODE HERE\n",
        "        argmax = np.argmax(x)\n",
        "        max_i = x[argmax]\n",
        "        exp_max_x = np.exp(x-max_i)\n",
        "        down = np.sum(exp_max_x)\n",
        "        for i in range(x.shape[0]):\n",
        "          up = exp_max_x[i]\n",
        "          soft_max_i = up / down\n",
        "          soft_max_list.append(soft_max_i)\n",
        "#         raise NotImplementedError\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    assert x.shape == np.array(soft_max_list).shape\n",
        "    return np.array(soft_max_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z0RTOTNVl9Mx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OBN4GrIqe5J5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forward_backward_prop(X, labels, params, dimensions):\n",
        "    \"\"\"\n",
        "    Forward and backward propagation for a two-layer sigmoidal network\n",
        "\n",
        "    Compute the forward propagation and for the cross entropy cost,\n",
        "    the backward propagation for the gradients for all parameters.\n",
        "\n",
        "    Notice the gradients computed here are different from the gradients in\n",
        "    the assignment sheet: they are w.r.t. weights, not inputs.\n",
        "\n",
        "    Arguments:\n",
        "    X -- M x Dx matrix, where each row is a training example x.\n",
        "    labels -- M x Dy matrix, where each row is a one-hot vector.\n",
        "    params -- Model parameters, these are unpacked for you.\n",
        "    dimensions -- A tuple of input dimension, number of hidden units\n",
        "                  and output dimension\n",
        "    \"\"\"\n",
        "\n",
        "    ### Unpack network parameters (do not modify)\n",
        "    ofs = 0\n",
        "    Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2])\n",
        "\n",
        "    W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H))\n",
        "    ofs += Dx * H\n",
        "    b1 = np.reshape(params[ofs:ofs + H], (1, H))\n",
        "    ofs += H\n",
        "    W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy))\n",
        "    ofs += H * Dy\n",
        "    b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy))\n",
        "\n",
        "    # Note: compute cost based on `sum` not `mean`.\n",
        "    ### YOUR CODE HERE: forward propagation\n",
        "    # raise NotImplementedError\n",
        "    a1 = sigmoid(np.dot(X, W1) + b1)\n",
        "    a2 = softmax(np.dot(a1, W2) + b2)\n",
        "    \n",
        "    cost = -np.sum(labels * np.log(a2))\n",
        "    # - np.sum(np.log(a2[labels == 1]))\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    ### YOUR CODE HERE: backward propagation\n",
        "    # raise NotImplementedError\n",
        "    loss_w = lambda w: cost\n",
        "    grad = {}\n",
        "    grad['w1'] = gradcheck_naive(loss_w,W1)\n",
        "    grad['b1'] = gradcheck_naive(loss_w,b1)\n",
        "    grad['w2'] = gradcheck_naive(loss_w,W2)\n",
        "    grad['b2'] = gradcheck_naive(loss_w,b1)\n",
        "    \n",
        "    ### END YOUR CODE\n",
        "\n",
        "    ### Stack gradients (do not modify)\n",
        "#     grad = np.concatenate((gradW1.flatten(), gradb1.flatten(),\n",
        "#         gradW2.flatten(), gradb2.flatten()))\n",
        "\n",
        "    return cost, grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cokbtw_Le5CC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def sanity_check():\n",
        "#     \"\"\"\n",
        "#     Set up fake data and parameters for the neural network, and test using\n",
        "#     gradcheck.\n",
        "#     \"\"\"\n",
        "#     print (\"Running sanity check...\")\n",
        "\n",
        "N = 20\n",
        "dimensions = [10, 5, 10]\n",
        "data = np.random.randn(N, dimensions[0])   # each row will be a datum\n",
        "labels = np.zeros((N, dimensions[2]))\n",
        "for i in range(N):\n",
        "    labels[i, random.randint(0,dimensions[2]-1)] = 1\n",
        "\n",
        "params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (\n",
        "    dimensions[1] + 1) * dimensions[2], )\n",
        "\n",
        "#     gradcheck_naive(lambda params:\n",
        "#         forward_backward_prop(data, labels, params, dimensions), params)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-eA0K-aCe46t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "56c64787-fc13-4ea1-c3e2-dcdcb8bdc19c"
      },
      "cell_type": "code",
      "source": [
        "forward_backward_prop(data, labels, params, dimensions)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(92.58644044107878, {'b1': 0.0, 'b2': 0.0, 'w1': 0.0, 'w2': 0.0})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "JLRL-pVPe4ye",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LaXCxSTxe4pZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}